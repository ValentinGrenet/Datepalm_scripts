{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 0 : Paths, Modules and Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% Files\n",
    "\n",
    "# Path to genome\n",
    "genome = '/home/valentin-grenet/Bureau/Données/Resources_yann/GCF_009389715.1_palm_55x_up_171113_PBpolish2nd_filt_p_genomic.fna'\n",
    "# Path to RepeatMasker hits of all hits (rare + common TEs)\n",
    "Repeat_all_data = '/home/valentin-grenet/Bureau/Données/Resources_yann/Repeat_LTR_all.out'\n",
    "# Path to RepeatMasker hits of cdhit (commons TEs, used to make the consensus sequences)\n",
    "Repeat_cdhit_data = '/home/valentin-grenet/Bureau/Données/Resources_yann/Repeat_LTR_cdhit.out'\n",
    "# Path to Inpactor hits of all hits (rare + common TEs)\n",
    "initial_library = '/home/valentin-grenet/Bureau/Données/Resources_yann/initial_library.fasta'\n",
    "# Path to Inpactor hits of cdhit (commons TEs, used to make the consensus sequences)\n",
    "final_library = '/home/valentin-grenet/Bureau/Données/Resources_yann/Final_library.fasta'\n",
    "# Path to sequences of genome contigs\n",
    "contig_dir = '/home/valentin-grenet/Bureau/Données/genome_sequences'\n",
    "# Path to sequences of consensus\n",
    "sequences_dir = '/home/valentin-grenet/Bureau/Données/LTR_cdhit_sequences'\n",
    "# Path to libraries\n",
    "libraries_dir = '/home/valentin-grenet/Bureau/Données/Gene_libraries/'\n",
    "# Path to Dante classification\n",
    "Dante_data = '/home/valentin-grenet/Bureau/Données/Resources_yann/DANTE_initial_like.txt'\n",
    "# Path to output directory\n",
    "results_dir = '/home/valentin-grenet/Bureau/Données/cdhit_classification'\n",
    "# Path to MrBayes\n",
    "mb = \"/home/valentin-grenet/Bureau/Outils/MrBayes/src/mb\"\n",
    "\n",
    "\n",
    "#%% Modules\n",
    "\n",
    "import os\n",
    "import subprocess\n",
    "\n",
    "from Bio import SeqIO                           # Used to have a uniform interface for input and output sequence file formats\n",
    "from Bio.Seq import Seq\n",
    "from Bio.SeqRecord import SeqRecord             # SeqRecord will be the object of the sequence file during the analysis in code\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1 : Extraction of RepeatMasker hit coordinates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ExtractCoordinatesRepeat(file, dico_hits):     # file = masking_hits\n",
    "    '''Function used to extract all hit coordinates obtained with RepeatMasker. Those hits are stored followingly :\n",
    "    dico_repeat_hits = {\"consensus1\" : [{hit1}, {hit2}, {hit3}, ...], \"consensus2\" : [{hit1}, {hit2}, {hit3}, ...], ...}\n",
    "    hit represents a dictionnary storing informations about a RepeatMasker hit as the following example :\n",
    "    {'contig': 'NC_016740.1', 'contig_start': '102549', 'contig_stop': '103252', 'strand': 'C', 'TE_start': '5985', 'TE_stop': '5278', 'perc_id': 62.0, 'index': 3}'''\n",
    "\n",
    "    index=0\n",
    "    for line in open(file, \"r\"):\n",
    "        index+=1                # line indices of the hit in the RepeatMasker hits file (begin with 1)\n",
    "        hit = {}\n",
    "        if line[0]==\"#\":    # skip header lines\n",
    "            continue\n",
    "\n",
    "        column = []\n",
    "        elements = line.split(\" \")\n",
    "        for i in range(0,len(elements)):\n",
    "            if elements[i] != \"\":\n",
    "                column.append(elements[i])      # adding each column of the tsv file in list\n",
    "        if column[9] not in dico_hits:      # add the consensus to dico_repeat_hits\n",
    "            dico_hits[column[9]] = []\n",
    "            # print(column[9])\n",
    "        hit[\"contig\"] = column[4]\n",
    "        hit[\"contig_start\"] = column[5]\n",
    "        hit[\"contig_stop\"] = column[6]\n",
    "        hit[\"strand\"] = column[8]\n",
    "        strand = 0\n",
    "        if hit[\"strand\"] == \"C\":            # used to manage TE_start and TE_stop (reversed with reverse strand)\n",
    "            strand = 1\n",
    "        hit[\"TE_start\"] = column[11+strand]\n",
    "        hit[\"TE_stop\"] = column[12+strand]\n",
    "        hit[\"perc_id\"] = 100 - sum(map(float, column[1:3]))\n",
    "        hit[\"index\"] = index\n",
    "        dico_hits[column[9]].append(hit)        # append the hit to repeat_dico_hits for the right consensus\n",
    "    return(dico_hits)\n",
    "\n",
    "dico_repeat_hits = {}\n",
    "# dico_repeat_hits = ExtractCoordinatesRepeat(Repeat_all_data, dico_repeat_hits)\n",
    "dico_repeat_hits = ExtractCoordinatesRepeat(Repeat_cdhit_data, dico_repeat_hits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if you want to check the dictionnary, change the consensus variable with a proper name\n",
    "\n",
    "consensus = \"consensus_Cluster_67_subfam_2\"\n",
    "for hit in dico_repeat_hits[consensus]:\n",
    "    print(hit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2 : Extraction of sequences (LTRs, genome and consensus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ExtractLengths(consensus, file, dico_sequences):\n",
    "    '''Extract the length of a sequence from a fasta file to a dictionnary of consensus names as keys.\n",
    "    The consensus name as to be provided with an existing dictionnary'''\n",
    "    dico_sequences[consensus] = 0\n",
    "    if file in os.listdir('.'):\n",
    "        for line in open(file, \"r\"):\n",
    "            if line[0] != \">\":\n",
    "                dico_sequences[consensus] += len(line)-1\n",
    "    return dico_sequences\n",
    "\n",
    "# dico_genome = ExtractSequences(genome, {})\n",
    "# dico_genome = {seq_record.id: seq_record for seq_record in SeqIO.parse(genome, \"fasta\")}          # dictionnary of contig sequences, not used here\n",
    "# dico_TEs = ExtractSequences(initial_library, {})\n",
    "\n",
    "os.chdir(sequences_dir)\n",
    "dico_consensus = {}; dico_5LTR = {}; dico_RT = {}; dico_RH = {}; dico_PROT = {}; dico_INT = {}; dico_GAG = {}; dico_3LTR = {}\n",
    "for consensus in os.listdir('.'):           # need a directory with only consensus directories\n",
    "    if \"c\" in consensus:\n",
    "        os.chdir(consensus)\n",
    "        if \"c\" in consensus:\n",
    "            dico_consensus = ExtractLengths(consensus, consensus + \".fasta\", dico_consensus)\n",
    "            dico_5LTR = ExtractLengths(consensus, consensus + \".5-LTR.fasta\", dico_5LTR)\n",
    "            dico_RT = ExtractLengths(consensus, consensus + \".RT.fasta\", dico_RT)\n",
    "            dico_RH = ExtractLengths(consensus, consensus + \".RH.fasta\", dico_RH)\n",
    "            dico_PROT = ExtractLengths(consensus, consensus + \".PROT.fasta\", dico_PROT)\n",
    "            dico_INT = ExtractLengths(consensus, consensus + \".RT.fasta\", dico_INT)\n",
    "            dico_GAG = ExtractLengths(consensus, consensus + \".GAG.fasta\", dico_GAG)\n",
    "            dico_3LTR = ExtractLengths(consensus, consensus + \".3-LTR.fasta\", dico_3LTR)\n",
    "        os.chdir(\"..\")\n",
    "\n",
    "dico_length = {}        # dico_length = for each consensus as key, a dictionnary with the length of all elements (genes, total, LTRs)\n",
    "for consensus in dico_consensus:\n",
    "    dico_length[consensus] = {\"complete\":dico_consensus[consensus], \n",
    "                              \"5-LTR\":dico_5LTR[consensus],\n",
    "                              \"RT\":dico_RT[consensus],\n",
    "                              \"RH\":dico_RH[consensus],\n",
    "                              \"PROT\":dico_PROT[consensus],\n",
    "                              \"INT\":dico_INT[consensus],\n",
    "                              \"GAG\":dico_GAG[consensus],\n",
    "                              \"3-LTR\":dico_3LTR[consensus],}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if you want to check your dictionnary\n",
    "\n",
    "for consensus in dico_consensus:\n",
    "    print(consensus, dico_length[consensus])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3 : Classification of LTRs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 3.1 : Simple recognition - Classification with length of LTR and full TE\n",
    "This first version only requires the length of elements to classify them.\n",
    "The initial percentage of coverage accepted is 80 % (0.8 in CheckLength)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CheckLength(hit, length, count):\n",
    "    '''From the length of the TE hit provided and the expected lengths of LTRs and complete\n",
    "    consensus sequence, the function proceeds to the classifcation of the hit.\n",
    "    - trunc : hit too short compared to LTR length\n",
    "    - paired : hit long enough to match complete sequence length, so considered with both LTRs\n",
    "    - single : hit too short to have both LTRs, but a great amount of internal sequence\n",
    "    - solo : hit with one LTR and some internal sequence'''\n",
    "    length_hit = int(hit[\"stop\"]) - int(hit[\"start\"])\n",
    "    if length_hit < 0.8*length[\"5-LTR\"]:\n",
    "        count[0] += 1\n",
    "        return \"trunc\", count\n",
    "    elif length_hit > 0.8*length[\"complete\"]:\n",
    "        count[3] += 1\n",
    "        return \"paired\", count\n",
    "    elif length_hit > 0.8*(length[\"complete\"]-length[\"5-LTR\"]):\n",
    "        count[2] += 1\n",
    "        return \"single\", count\n",
    "    else:\n",
    "        count[1] += 1\n",
    "        return \"solo\", count\n",
    "\n",
    "os.chdir(sequences_dir)\n",
    "headers = [\"index\", \"contig\", \"start\", \"stop\", \"length\", \"strand\", \"element\"]        # informations given for each hit in output files\n",
    "dico_count = {}\n",
    "\n",
    "for consensus in dico_repeat_hits :\n",
    "    print(consensus)\n",
    "    os.chdir(consensus)\n",
    "    output_consensus = open(consensus + \".LTR_types.tsv\", \"w\")\n",
    "    output_consensus.write(\"\\t\".join(headers) + \"\\n\")\n",
    "    count = [0,0,0,0]             # count the number of paired, single, solo and truncated elements\n",
    "    \n",
    "    for hit in dico_repeat_hits[consensus]:\n",
    "        status, count = CheckLength(hit, dico_length[consensus], count)         # status = LTR classification\n",
    "        line = [str(hit[\"index\"]), hit[\"contig\"], hit[\"start\"], hit[\"stop\"], str(int(hit[\"stop\"])-int(hit[\"start\"])), hit[\"strand\"], status]\n",
    "        output_consensus.write(\"\\t\".join(line)+\"\\n\")\n",
    "    \n",
    "    output_consensus.close()\n",
    "    dico_count[consensus] = count           # one count list for each consensus\n",
    "    os.chdir(\"..\")\n",
    "\n",
    "output_count = open(\"annotation_counts.tsv\", \"w\")\n",
    "headers = [\"consensus\", \"length\", \"trunc\", \"solo\", \"single\", \"paired\"]\n",
    "output_count.write(\"\\t\".join(headers) + \"\\n\")\n",
    "\n",
    "for consensus in dico_count:\n",
    "    output_count.write(consensus + \"\\t\" + str(dico_length[consensus][\"complete\"]) + \"\\t\" + \"\\t\".join(str(i) for i in dico_count[consensus]) + \"\\n\")   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 3.2 : Advanced recognition - Classification with length and coordinates\n",
    "This second version is still using the length of elements to classify each hits, but also use coordinates of hits to identify the presence or not of LTRs, providing a more efficient classification, especially for paired ones. However, some problems remain (as taking the strand sense into account) and the next version is the more efficient.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ClassLTR(hit, consensus_length, consensus_hits, count):\n",
    "    '''From the length of the TE hit provided and the expected lengths of LTRs and complete\n",
    "    consensus sequence, the function proceeds to the classifcation of the hit.\n",
    "    The coordinates of hit with the TE are also taken into account to precise the classification.\n",
    "    - complete (paired in 3.1) : hit long enough to match complete sequence length, so considered with both LTRs\n",
    "    if not complete, presence of LTRs is checked.\n",
    "    - paired : two complete LTRs have been identified with 2 different hits\n",
    "    - single : one LTR identified, and also some internal sequence\n",
    "    - solo : one LTR identified but not much internal sequence with it\n",
    "    - trunc : hit too short compared to LTR length'''\n",
    "    if CheckComplete(hit, consensus_length):\n",
    "        count[4] += 1\n",
    "        return [hit], \"complete\", count\n",
    "    else:\n",
    "        if CheckLTR(hit, consensus_length):\n",
    "            if CheckPaired(hit, consensus_length, consensus_hits):\n",
    "                TE_hits = BuildTE(hit, consensus_length, consensus_hits)\n",
    "                count[3] += 1\n",
    "                return TE_hits, \"paired\", count\n",
    "            else:\n",
    "                if CheckSingle(hit, consensus_length, consensus_hits):\n",
    "                    TE_hits = BuildTE(hit, consensus_length, consensus_hits)\n",
    "                    count[2] += 1\n",
    "                    return TE_hits, \"single\", count\n",
    "                else:\n",
    "                    count[1] += 1\n",
    "                    return [hit], \"solo\", count\n",
    "        else:\n",
    "            count[0] += 1\n",
    "            return [], \"trunc\", count\n",
    "\n",
    "def CheckComplete(hit, consensus_length):\n",
    "    '''Check the length of the hit and compare hit to the consensus complete sequence length.\n",
    "    Coverage percentage of 80 % minimum.'''\n",
    "    length_hit = int(hit[\"contig_stop\"]) - int(hit[\"contig_start\"])\n",
    "    return length_hit >= 0.8*consensus_length[\"complete\"]\n",
    "\n",
    "def CheckLTR(hit, consensus_length):\n",
    "    '''Based on TE hit coordinates (with what matches the hit on the TE),\n",
    "    we estimate the coverage of a single LTR based on its length. If the coverage is sufficient\n",
    "    (75 % of identity on 50 % of the LTR), we consider that the LTR as been covered'''\n",
    "    if hit[\"strand\"] == \"+\" :\n",
    "        # testing if the coordinates are matching with an LTR covering and not an internal sequence covering\n",
    "        if int(hit[\"TE_start\"]) <= consensus_length[\"5-LTR\"] \\\n",
    "        or int(hit[\"TE_stop\"]) >= consensus_length[\"complete\"]-consensus_length[\"3-LTR\"]:\n",
    "            # testing if the identity and coverage are sufficient\n",
    "            return int(hit[\"TE_stop\"])-int(hit[\"TE_start\"]) >= 0.5*consensus_length[\"5-LTR\"] \\\n",
    "                and int(hit[\"perc_id\"]) >= 75\n",
    "        else:\n",
    "            return False\n",
    "    else:\n",
    "        # testing if the coordinates are matching with an LTR covering and not an internal sequence covering\n",
    "        if int(hit[\"TE_stop\"]) <= consensus_length[\"5-LTR\"] \\\n",
    "        or int(hit[\"TE_start\"]) >= consensus_length[\"complete\"]-consensus_length[\"3-LTR\"]:\n",
    "            # testing if the identity and coverage are sufficient\n",
    "            return int(hit[\"TE_start\"])-int(hit[\"TE_stop\"]) >= 0.5*consensus_length[\"5-LTR\"] \\\n",
    "                and int(hit[\"perc_id\"]) >= 75\n",
    "        else:\n",
    "            return False\n",
    "\n",
    "def CheckPaired(query_hit, consensus_length, consensus_hits):\n",
    "    '''From the first LTR identified, and according to its strand sense and its classification (5' or 3'),\n",
    "    we are searching the second LTR.\n",
    "    Not working because of errors with the strand sense.'''\n",
    "    if int(query_hit[\"TE_start\"]) < consensus_length[\"5-LTR\"] : \n",
    "        if query_hit[\"strand\"] == \"+\" :\n",
    "            coord_start = int(query_hit[\"contig_start\"]) - int(query_hit[\"TE_start\"])\n",
    "        else:\n",
    "            coord_start = int(query_hit[\"contig_start\"]) + int(query_hit[\"TE_start\"]) - consensus_length[\"complete\"]\n",
    "        # coord_min = coord_start + consensus_length[\"complete\"] - (1.2*consensus_length[\"3-LTR\"])\n",
    "        coord_max = coord_start + consensus_length[\"complete\"] + (0.2*consensus_length[\"3-LTR\"])\n",
    "        print(query_hit[\"index\"], coord_start, coord_max)\n",
    "        for other_hit in consensus_hits:\n",
    "            if coord_start <= int(other_hit[\"contig_stop\"]) <= coord_max \\\n",
    "            and query_hit[\"contig\"] == other_hit[\"contig\"] \\\n",
    "            and query_hit[\"index\"] != other_hit[\"index\"]:               # added for smaller TE paired anyway (linked with suppression of coord_min)\n",
    "                if CheckLTR(other_hit, consensus_length):\n",
    "                    return True\n",
    "    else:                                                                   # else we have the 3-LTR\n",
    "        if query_hit[\"strand\"] == \"+\" :\n",
    "            coord_start = int(query_hit[\"contig_stop\"]) - int(query_hit[\"TE_stop\"]) + consensus_length[\"complete\"]\n",
    "        else:\n",
    "            coord_start = int(query_hit[\"contig_stop\"]) + int(query_hit[\"TE_stop\"])\n",
    "        coord_min = coord_start - consensus_length[\"complete\"] - (0.2*consensus_length[\"3-LTR\"])\n",
    "        # coord_max = coord_start - consensus_length[\"complete\"] + (1.2*consensus_length[\"3-LTR\"])\n",
    "        print(query_hit[\"index\"], coord_start, coord_min)\n",
    "        for other_hit in consensus_hits:\n",
    "            if coord_min <= int(other_hit[\"contig_start\"]) <= coord_start \\\n",
    "            and query_hit[\"contig\"] == other_hit[\"contig\"] \\\n",
    "            and query_hit[\"index\"] != other_hit[\"index\"]:               # added for smaller TE paired anyway (linked with suppression of coord_max)\n",
    "                if CheckLTR(other_hit, consensus_length):\n",
    "                    return True\n",
    "    return False\n",
    "\n",
    "def CheckSingle(query_hit, consensus_length, consensus_hits):\n",
    "    length_hit = int(hit[\"contig_stop\"]) - int(hit[\"contig_start\"])\n",
    "    if length_hit >= consensus_length[\"5-LTR\"]+500:\n",
    "        return True\n",
    "    else:\n",
    "        if int(query_hit[\"TE_start\"]) < consensus_length[\"5-LTR\"] :          # change due to error of pairing with itself\n",
    "            if query_hit[\"strand\"] == \"+\" :\n",
    "                coord_start = int(query_hit[\"contig_start\"]) - int(query_hit[\"TE_start\"]) + consensus_length[\"5-LTR\"]\n",
    "            else:\n",
    "                coord_start = int(query_hit[\"contig_start\"]) + int(query_hit[\"TE_start\"]) - consensus_length[\"complete\"] + consensus_length[\"5-LTR\"]\n",
    "            for other_hit in consensus_hits:\n",
    "                if coord_start <= int(other_hit[\"contig_start\"]) <= coord_start+500 \\\n",
    "                and query_hit[\"contig\"] == other_hit[\"contig\"]:\n",
    "                    return True\n",
    "        else:\n",
    "            if query_hit[\"strand\"] == \"+\" :\n",
    "                coord_start = int(query_hit[\"contig_stop\"]) - int(query_hit[\"TE_stop\"]) + consensus_length[\"complete\"] - consensus_length[\"5-LTR\"]\n",
    "            else:\n",
    "                coord_start = int(query_hit[\"contig_stop\"]) + int(query_hit[\"TE_stop\"]) - consensus_length[\"5-LTR\"]\n",
    "            for other_hit in consensus_hits:\n",
    "                if coord_start-500 <= int(other_hit[\"contig_stop\"]) <= coord_start \\\n",
    "                and query_hit[\"contig\"] == other_hit[\"contig\"]:\n",
    "                    return True\n",
    "        return False\n",
    "\n",
    "def BuildTE(query_hit, consensus_length, consensus_hits):\n",
    "    TE_hits = []\n",
    "    if int(query_hit[\"TE_start\"]) < consensus_length[\"5-LTR\"] :          # change due to error of pairing with itself\n",
    "        if query_hit[\"strand\"] == \"+\" :\n",
    "            coord_start = int(query_hit[\"contig_start\"]) - int(query_hit[\"TE_start\"])\n",
    "        else:\n",
    "            coord_start = int(query_hit[\"contig_start\"]) + int(query_hit[\"TE_start\"]) - consensus_length[\"complete\"]\n",
    "        for other_hit in consensus_hits:\n",
    "            # print(other_hit[\"contig_start\"])\n",
    "            if coord_start <= int(other_hit[\"contig_start\"]) <= coord_start+1.2*consensus_length[\"complete\"] \\\n",
    "            and query_hit[\"contig\"] == other_hit[\"contig\"]:\n",
    "                TE_hits.append(other_hit)\n",
    "    else:\n",
    "        if query_hit[\"strand\"] == \"+\" :\n",
    "            coord_start = int(query_hit[\"contig_stop\"]) - int(query_hit[\"TE_stop\"]) + consensus_length[\"complete\"]\n",
    "        else:\n",
    "            coord_start = int(query_hit[\"contig_stop\"]) + int(query_hit[\"TE_stop\"])\n",
    "        for other_hit in consensus_hits:\n",
    "            if coord_start-1.2*consensus_length[\"complete\"] <= int(other_hit[\"contig_stop\"]) <= coord_start \\\n",
    "            and query_hit[\"contig\"] == other_hit[\"contig\"]:\n",
    "                TE_hits.append(other_hit)\n",
    "    return TE_hits\n",
    "\n",
    "\n",
    "\n",
    "os.chdir(sequences_dir)\n",
    "headers = [\"index\", \"contig\", \"start\", \"stop\", \"length\", \"strand\", \"id_percentage\", \"TE_start\", \"TE_stop\", \"classification\", \"index\", \"start\", \"stop\", \"length\", \"strand\", \"id_percentage\", \"TE_start\", \"TE_stop\"]\n",
    "dico_count = {}\n",
    "\n",
    "for consensus in dico_repeat_hits :\n",
    "    print(consensus)\n",
    "    os.chdir(consensus)\n",
    "    output_consensus = open(consensus + \".LTR_classif.tsv\", \"w\")\n",
    "    output_consensus.write(\"\\t\".join(headers) + \"\\n\")\n",
    "    count = [0,0,0,0,0]\n",
    "    hits_used = []\n",
    "    \n",
    "    for hit in dico_repeat_hits[consensus]:\n",
    "        if hit in hits_used:\n",
    "            continue\n",
    "        else:\n",
    "            TE_hits, status, count = ClassLTR(hit, dico_length[consensus], dico_repeat_hits[consensus], count)\n",
    "            line = [str(hit[\"index\"]), hit[\"contig\"], hit[\"contig_start\"], hit[\"contig_stop\"], str(int(hit[\"contig_stop\"])-int(hit[\"contig_start\"])), hit[\"strand\"], str(hit[\"perc_id\"]), hit[\"TE_start\"], hit[\"TE_stop\"], status]\n",
    "            output_consensus.write(\"\\t\".join(line))\n",
    "            if len(TE_hits) == 0 :\n",
    "                output_consensus.write('\\tno match\\n')\n",
    "            else:\n",
    "                new_line = '\\t'\n",
    "                for other_hit in TE_hits:\n",
    "                    hits_used.append(other_hit)\n",
    "                    line = [str(other_hit[\"index\"]), other_hit[\"contig_start\"], other_hit[\"contig_stop\"], str(int(other_hit[\"contig_stop\"])-int(other_hit[\"contig_start\"])), other_hit[\"strand\"], str(other_hit[\"perc_id\"]), other_hit[\"TE_start\"], other_hit[\"TE_stop\"]]\n",
    "                    output_consensus.write(new_line + \"\\t\".join(line) + \"\\n\")\n",
    "                    new_line = 10*'\\t'\n",
    "\n",
    "    \n",
    "    output_consensus.close()\n",
    "    dico_count[consensus] = count\n",
    "    os.chdir(\"..\")\n",
    "\n",
    "output_count = open(\"annotation_counts.tsv\", \"w\")\n",
    "headers = [\"consensus\", \"length\", \"trunc\", \"solo\", \"single\", \"paired\", \"complete\"]\n",
    "output_count.write(\"\\t\".join(headers) + \"\\n\")\n",
    "\n",
    "for consensus in dico_count:\n",
    "    output_count.write(consensus + \"\\t\" + str(dico_length[consensus][\"complete\"]) + \"\\t\" + \"\\t\".join(str(i) for i in dico_count[consensus]) + \"\\n\")   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 3.3 : Major updates on recognition\n",
    "\n",
    "The previous script had interessant results, so we kept the same method and brought ameliorations :\n",
    "- for reverse strand sense, uncomplete fragment then added for a single or a paired LTR are still written as trunc -> option to keep them in memory, and then write them as trunc or in the paired or single LTR, and modify the count list\n",
    "- add the option to recheck the completion of the LTR analyzed to pass it from paired to complete (or fragmented)\n",
    "- manage the cases where the LTR is complete, but too small to be considered as a complete (internal sequences truncated) --> considered as shrunked\n",
    "- change of management between LTR scan on the left or on the right, because it was not working well before\n",
    "\n",
    "comment on the efficiency :\n",
    "the script will never be perfect as the threshold will always miss some LTR which should be considered as a classification above (false negative) and consider some LTR in a classification above where they should be (false positive)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SearchSense(strand, TE_start, TE_stop, LTR_length):\n",
    "    '''Function to define the sense of research of the second LTR'''\n",
    "    sign = strand == \"+\"\n",
    "    number = TE_start < LTR_length or TE_stop < LTR_length\n",
    "    if sign and number:\n",
    "        return True\n",
    "    elif not sign and not number:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "    \n",
    "def FixCount(status, count):\n",
    "    '''Function to uncount a smaller hit first counted but then taken by a reversed longer hit\n",
    "    (example : a trunc hit is part of the internal sequence of a paired hit)'''\n",
    "    list_status = [\"trunc\", \"solo\", \"single\", \"paired\", \"fragmented\", \"shrunked\", \"complete\"]\n",
    "    count[list_status.index(status)] -= 1\n",
    "    return count\n",
    "\n",
    "def ClassLTR(hit, consensus_length, consensus_hits, count):\n",
    "    '''From the length of the TE hit provided and the expected lengths of LTRs and complete\n",
    "    consensus sequence, the function proceeds to the classifcation of the hit.\n",
    "    The coordinates of hit with the TE are also taken into account to precise the classification.\n",
    "    - complete : hit long enough to match complete sequence length, so considered with both LTRs\n",
    "    if not complete, presence of LTRs is checked.\n",
    "    - shrunked : hit not long enough to be considered as complete, but the single hit as both LTRs\n",
    "    according to its TE hit coordinates (case of internal sequence shortened)\n",
    "    - fragmented : two complete LTRs with 2 different hits, and internal sequence is \n",
    "    sufficiently covered as well (so a complete TE on several hits)\n",
    "    - paired : two complete LTRs have been identified with 2 different hits, but internal sequence is lacking\n",
    "    - single : one LTR identified, and also some internal sequence\n",
    "    - solo : one LTR identified but not much internal sequence with it\n",
    "    - trunc : hit too short compared to LTR length'''\n",
    "    if CheckComplete(hit, consensus_length):\n",
    "        count[6] += 1\n",
    "        return [hit], \"complete\", count\n",
    "    elif CheckShrunked(hit, consensus_length):            # peut-être clippé\n",
    "        count[5] += 1\n",
    "        return [hit], \"shrunked\", count\n",
    "    elif CheckLTR(hit, consensus_length):\n",
    "        if CheckPaired(hit, consensus_length, consensus_hits):\n",
    "            TE_hits = BuildTE(hit, consensus_length, consensus_hits)\n",
    "            if CheckFragmented(TE_hits, consensus_length):\n",
    "                count[4] += 1\n",
    "                return TE_hits, \"fragmented\", count\n",
    "            else:\n",
    "                count[3] += 1\n",
    "                return TE_hits, \"paired\", count\n",
    "        elif CheckSingle(hit, consensus_length, consensus_hits):\n",
    "            TE_hits = BuildTE(hit, consensus_length, consensus_hits)\n",
    "            count[2] += 1\n",
    "            return TE_hits, \"single\", count\n",
    "        else:\n",
    "            count[1] += 1\n",
    "            return [hit], \"solo\", count\n",
    "    else:\n",
    "        count[0] += 1\n",
    "        return [], \"trunc\", count\n",
    "\n",
    "def CheckComplete(hit, consensus_length):\n",
    "    '''Check the length of the hit and compare hit to the consensus complete sequence length.\n",
    "    Coverage percentage of 80 % minimum.'''\n",
    "    length_hit = int(hit[\"contig_stop\"]) - int(hit[\"contig_start\"])\n",
    "    return length_hit >= 0.8*consensus_length[\"complete\"]\n",
    "\n",
    "def CheckShrunked(hit, consensus_length):\n",
    "    '''After being checked as not complete (not long enough), we verify if the hit has both LTRs anyway\n",
    "    (case of truncated internal sequence, but both LTRs are present)'''\n",
    "    coord_5LTR = 0.5*consensus_length[\"5-LTR\"]\n",
    "    coord_3LTR = consensus_length[\"complete\"] - 0.5*consensus_length[\"3-LTR\"]\n",
    "    if hit[\"strand\"] == \"+\":\n",
    "        return int(hit[\"TE_start\"]) < coord_5LTR and int(hit[\"TE_stop\"]) > coord_3LTR\n",
    "    else:\n",
    "        return int(hit[\"TE_stop\"]) < coord_5LTR and int(hit[\"TE_start\"]) > coord_3LTR\n",
    "\n",
    "def CheckLTR(hit, consensus_length):\n",
    "    '''Based on TE hit coordinates (with what matches the hit on the TE),\n",
    "    we estimate the coverage of a single LTR based on its length. If the coverage is sufficient\n",
    "    (75 % of identity on 50 % of the LTR), we consider that the LTR as been covered'''\n",
    "    if hit[\"strand\"] == \"+\" :\n",
    "        # testing if the coordinates are matching with an LTR covering and not an internal sequence covering\n",
    "        if int(hit[\"TE_start\"]) <= consensus_length[\"5-LTR\"] \\\n",
    "        or int(hit[\"TE_stop\"]) >= consensus_length[\"complete\"]-consensus_length[\"3-LTR\"]:\n",
    "            # testing if the identity and coverage are sufficient\n",
    "            return int(hit[\"TE_stop\"])-int(hit[\"TE_start\"]) >= 0.5*consensus_length[\"5-LTR\"] \\\n",
    "                and int(hit[\"perc_id\"]) >= 75\n",
    "        else:\n",
    "            return False\n",
    "    else:\n",
    "        # testing if the coordinates are matching with an LTR covering and not an internal sequence covering\n",
    "        if int(hit[\"TE_stop\"]) <= consensus_length[\"5-LTR\"] \\\n",
    "        or int(hit[\"TE_start\"]) >= consensus_length[\"complete\"]-consensus_length[\"3-LTR\"]:\n",
    "            # testing if the identity and coverage are sufficient\n",
    "            return int(hit[\"TE_start\"])-int(hit[\"TE_stop\"]) >= 0.5*consensus_length[\"5-LTR\"] \\\n",
    "                and int(hit[\"perc_id\"]) >= 75\n",
    "        else:\n",
    "            return False\n",
    "\n",
    "def CheckPaired(query_hit, consensus_length, consensus_hits):\n",
    "    '''From the first LTR identified with the hit coordinates, we build an interval of coordinates\n",
    "    on the genome in which we have to find the second LTR (so use of CheckLTR for any hit found in this interval)'''\n",
    "    # case of searching the second LTR on the right (3'LTR for a 5'LTR on +, or 5'LTR for a 3'LTR on -)\n",
    "    if SearchSense(query_hit[\"strand\"], int(query_hit[\"TE_start\"]), int(query_hit[\"TE_stop\"]), consensus_length[\"5-LTR\"]):\n",
    "        # coord_start = estimated first base of 5'LTR or last base of 3'LTR from TE and contig coordinates\n",
    "        if query_hit[\"strand\"] == \"+\" :\n",
    "            coord_start = int(query_hit[\"contig_start\"]) - int(query_hit[\"TE_start\"])\n",
    "        else:\n",
    "            coord_start = int(query_hit[\"contig_start\"]) + int(query_hit[\"TE_start\"]) - consensus_length[\"complete\"]\n",
    "        coord_min = coord_start + consensus_length[\"complete\"] - (1.2*consensus_length[\"3-LTR\"])\n",
    "        coord_max = coord_start + consensus_length[\"complete\"] + (1.2*consensus_length[\"3-LTR\"])\n",
    "        for other_hit in consensus_hits:\n",
    "            if coord_min <= int(other_hit[\"contig_stop\"]) <= coord_max \\\n",
    "            and query_hit[\"contig\"] == other_hit[\"contig\"] \\\n",
    "            and query_hit[\"index\"] != other_hit[\"index\"]:               # avoid pairing with itself\n",
    "                if CheckLTR(other_hit, consensus_length):\n",
    "                    return True\n",
    "    # case of searching the second LTR on the left (5'LTR for a 3'LTR on +, or 3'LTR for a 5'LTR on -)\n",
    "    else:\n",
    "        # coord_start = estimated first base of 5'LTR or last base of 3'LTR from TE and contig coordinates\n",
    "        if query_hit[\"strand\"] == \"+\" :\n",
    "            coord_start = int(query_hit[\"contig_stop\"]) - int(query_hit[\"TE_stop\"]) + consensus_length[\"complete\"]\n",
    "        else:\n",
    "            coord_start = int(query_hit[\"contig_stop\"]) + int(query_hit[\"TE_stop\"])\n",
    "        coord_min = coord_start - consensus_length[\"complete\"] - (1.2*consensus_length[\"3-LTR\"])\n",
    "        coord_max = coord_start - consensus_length[\"complete\"] + (1.2*consensus_length[\"3-LTR\"])\n",
    "        for other_hit in consensus_hits:\n",
    "            if coord_min <= int(other_hit[\"contig_start\"]) <= coord_max \\\n",
    "            and query_hit[\"contig\"] == other_hit[\"contig\"] \\\n",
    "            and query_hit[\"index\"] != other_hit[\"index\"]:               # avoid pairing with itself\n",
    "                if CheckLTR(other_hit, consensus_length):\n",
    "                    return True\n",
    "    return False\n",
    "\n",
    "def CheckFragmented(TE_hits, consensus_length):\n",
    "    '''Check if the collection of hits building one TE is covering a great amount of internal sequence\n",
    "    (coverage > 80 % as a complete TE)'''\n",
    "    length_covered = 0\n",
    "    for hit in TE_hits:\n",
    "        if length_covered == 0:\n",
    "            length_covered = int(hit[\"contig_stop\"]) - int(hit[\"contig_start\"])\n",
    "            coord_max = int(hit[\"contig_stop\"])\n",
    "        else:\n",
    "            length_covered += int(hit[\"contig_stop\"]) - max(coord_max, int(hit[\"contig_start\"]))\n",
    "    return length_covered >= 0.8*consensus_length[\"complete\"]\n",
    "\n",
    "def CheckSingle(query_hit, consensus_length, consensus_hits):\n",
    "    '''Check if the hit (or a near small hit) is covering some internal sequence in addition to its LTR'''\n",
    "    length_hit = int(hit[\"contig_stop\"]) - int(hit[\"contig_start\"])\n",
    "    if length_hit >= consensus_length[\"5-LTR\"]+500:\n",
    "        return True\n",
    "    else:\n",
    "        # case of searching a second hit on the right (5'LTR on +, or 3'LTR on -)\n",
    "        if SearchSense(query_hit[\"strand\"], int(query_hit[\"TE_start\"]), int(query_hit[\"TE_stop\"]), consensus_length[\"5-LTR\"]):\n",
    "            # coord_start = estimated last base of 5'LTR or first base of 3'LTR from TE and contig coordinates\n",
    "            if query_hit[\"strand\"] == \"+\" :\n",
    "                coord_start = int(query_hit[\"contig_start\"]) - int(query_hit[\"TE_start\"]) + consensus_length[\"5-LTR\"]\n",
    "            else:\n",
    "                coord_start = int(query_hit[\"contig_start\"]) + int(query_hit[\"TE_start\"]) - consensus_length[\"complete\"] + consensus_length[\"5-LTR\"]\n",
    "            for other_hit in consensus_hits:\n",
    "                if coord_start <= int(other_hit[\"contig_start\"]) <= coord_start+500 \\\n",
    "                and query_hit[\"contig\"] == other_hit[\"contig\"]:\n",
    "                    return True\n",
    "        # case of searching a second hit on the left (3'LTR on +, or 5'LTR on -)\n",
    "        else:\n",
    "            # coord_start = estimated last base of 5'LTR or first base of 3'LTR from TE and contig coordinates\n",
    "            if query_hit[\"strand\"] == \"+\" :\n",
    "                coord_start = int(query_hit[\"contig_stop\"]) - int(query_hit[\"TE_stop\"]) + consensus_length[\"complete\"] - consensus_length[\"5-LTR\"]\n",
    "            else:\n",
    "                coord_start = int(query_hit[\"contig_stop\"]) + int(query_hit[\"TE_stop\"]) - consensus_length[\"5-LTR\"]\n",
    "            for other_hit in consensus_hits:\n",
    "                if coord_start-500 <= int(other_hit[\"contig_stop\"]) <= coord_start \\\n",
    "                and query_hit[\"contig\"] == other_hit[\"contig\"]:\n",
    "                    return True\n",
    "        return False\n",
    "\n",
    "def BuildTE(query_hit, consensus_length, consensus_hits):\n",
    "    '''From the first hit identified with hit coordinates, we build an interval of coordinates\n",
    "    on the genome (corresponding to the complete TE length), in which we find other hits of the\n",
    "    same consensus. The query and found hits are considered building the same TE.'''\n",
    "    TE_hits = []            # list of hits added with the query hit\n",
    "    # case of searching a hits on the right (5'LTR on +, or 3'LTR on -)\n",
    "    if SearchSense(query_hit[\"strand\"], int(query_hit[\"TE_start\"]), int(query_hit[\"TE_stop\"]), consensus_length[\"5-LTR\"]):\n",
    "        # coord_start = estimated first base of 5'LTR or last base of 3'LTR from TE and contig coordinates\n",
    "        if query_hit[\"strand\"] == \"+\" :\n",
    "            coord_start = int(query_hit[\"contig_start\"]) - int(query_hit[\"TE_start\"])\n",
    "        else:\n",
    "            coord_start = int(query_hit[\"contig_start\"]) + int(query_hit[\"TE_start\"]) - consensus_length[\"complete\"]\n",
    "        for other_hit in consensus_hits:\n",
    "            # print(other_hit[\"contig_start\"])\n",
    "            if coord_start <= int(other_hit[\"contig_start\"]) <= coord_start+1.2*consensus_length[\"complete\"] \\\n",
    "            and query_hit[\"contig\"] == other_hit[\"contig\"]:\n",
    "                TE_hits.append(other_hit)\n",
    "    # case of searching a hits on the left (3'LTR on +, or 5'LTR on -)\n",
    "    else:\n",
    "        # coord_start = estimated first base of 5'LTR or last base of 3'LTR from TE and contig coordinates\n",
    "        if query_hit[\"strand\"] == \"+\" :\n",
    "            coord_start = int(query_hit[\"contig_stop\"]) - int(query_hit[\"TE_stop\"]) + consensus_length[\"complete\"]\n",
    "        else:\n",
    "            coord_start = int(query_hit[\"contig_stop\"]) + int(query_hit[\"TE_stop\"])\n",
    "        for other_hit in consensus_hits:\n",
    "            if coord_start-1.2*consensus_length[\"complete\"] <= int(other_hit[\"contig_stop\"]) <= coord_start \\\n",
    "            and query_hit[\"contig\"] == other_hit[\"contig\"]:\n",
    "                TE_hits.append(other_hit)\n",
    "    return TE_hits\n",
    "\n",
    "\n",
    "\n",
    "os.chdir(sequences_dir)\n",
    "# headers for the LTR classification files for each consensus\n",
    "headers = [\"index\", \"contig\", \"start\", \"stop\", \"length\", \"strand\", \"id_percentage\", \"TE_start\", \"TE_stop\", \"classification\", \"index\", \"start\", \"stop\", \"length\", \"strand\", \"id_percentage\", \"TE_start\", \"TE_stop\"]\n",
    "dico_count = {}\n",
    "\n",
    "for consensus in dico_repeat_hits :\n",
    "    print(consensus)\n",
    "    os.chdir(consensus)\n",
    "    output_consensus = open(consensus + \".LTR_classif.tsv\", \"w\")\n",
    "    output_consensus.write(\"\\t\".join(headers) + \"\\n\")\n",
    "    count = [0,0,0,0,0,0,0]             # count the number of each LTR class, one list per consensus\n",
    "    hits_used = []                      # used to avoid repetitions of hits written in the output files\n",
    "    list_lines = []                     # used to write the output file for each consensus (the list contains all lines for one consensus)\n",
    "    \n",
    "    for hit in dico_repeat_hits[consensus]:\n",
    "        # manage repetitions in + sense (a hit building another TE is in hits_used)\n",
    "        if hit in hits_used:\n",
    "            continue\n",
    "        else:\n",
    "            TE_hits, status, count = ClassLTR(hit, dico_length[consensus], dico_repeat_hits[consensus], count)\n",
    "            ref_line = [str(hit[\"index\"]), hit[\"contig\"], hit[\"contig_start\"], hit[\"contig_stop\"], str(int(hit[\"contig_stop\"])-int(hit[\"contig_start\"])), hit[\"strand\"], str(hit[\"perc_id\"]), hit[\"TE_start\"], hit[\"TE_stop\"], status]\n",
    "            list_lines.append(ref_line)\n",
    "            # in case of a trunc LTR\n",
    "            if len(TE_hits) == 0 :\n",
    "                list_lines.append([\"\\tno_match\\n\"])\n",
    "            else:\n",
    "                new_line = ''\n",
    "                for other_hit in TE_hits:\n",
    "                    hits_used.append(other_hit)\n",
    "                    # manage repetitions in - sense (smaller hits taking part in another TE and already written have to be deleted)\n",
    "                    for line in list_lines:\n",
    "                        if str(other_hit[\"index\"]) == line[0] and line != ref_line:         # ref_line already in the list, but is not a repetition\n",
    "                            del list_lines[list_lines.index(line):list_lines.index(line)+2]\n",
    "                            count = FixCount(line[-1], count)\n",
    "                    line = [new_line, str(other_hit[\"index\"]), other_hit[\"contig_start\"], other_hit[\"contig_stop\"], str(int(other_hit[\"contig_stop\"])-int(other_hit[\"contig_start\"])), other_hit[\"strand\"], str(other_hit[\"perc_id\"]), other_hit[\"TE_start\"], other_hit[\"TE_stop\"], \"\\n\"]\n",
    "                    list_lines.append(line)\n",
    "                    new_line = 9*'\\t'\n",
    "\n",
    "    for line in list_lines:\n",
    "        output_consensus.write(\"\\t\".join(line))\n",
    "    output_consensus.close()\n",
    "    dico_count[consensus] = count\n",
    "    os.chdir(\"..\")\n",
    "\n",
    "output_count = open(\"annotation_counts.tsv\", \"w\")\n",
    "headers = [\"consensus\", \"length\", \"trunc\", \"solo\", \"single\", \"paired\", \"fragmented\", \"shrunked\", \"complete\"]\n",
    "output_count.write(\"\\t\".join(headers) + \"\\n\")\n",
    "\n",
    "for consensus in dico_count:\n",
    "    output_count.write(consensus + \"\\t\" + str(dico_length[consensus][\"complete\"]) + \"\\t\" + \"\\t\".join(str(i) for i in dico_count[consensus]) + \"\\n\")   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 3.4 : Minor updates on recognition - 3.3 is working very well, so now we are making optimisations\n",
    "\n",
    "Here is the same script as previously, but it is free for some modifications to bring the ameliorations below.\n",
    "\n",
    "to do :\n",
    "- uncomplete fragments added after the end of the LTR --> count them as trunc, or keep them as they could be part of the TE ? (ex : 34971 for c67_s2)\n",
    "- sometimes, a complete fragment is considered as paired --> identify them with maybe a second check of complete, or the impossibility to add a too long fragment (ex : 154500 in consensus67_s2)\n",
    "- consider the paired LTR too close to be considered as paired (important lack of internal sequence) (ex : TE of 9000 instead of 12000 for 5346 in c67_s2), but manage the threshold to not paired the LTR with himself if the LTR is too long\n",
    "- parameters modified by the user\n",
    "\n",
    "done :\n",
    "- completion factor from 0.8 to 0.95 because for the next step, some complete fragments didn't have both LTR\n",
    "\n",
    "comment on the efficiency :\n",
    "the script will never be perfect as the threshold will always miss some LTR which should be considered as a classification above (false negative) and consider some LTR in a classification above where they should be (false positive)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SearchSense(strand, TE_start, TE_stop, LTR_length):\n",
    "    sign = strand == \"+\"\n",
    "    number = TE_start < LTR_length or TE_stop < LTR_length\n",
    "    if sign and number:\n",
    "        return True\n",
    "    elif not sign and not number:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "    \n",
    "def FixCount(status, count):\n",
    "    list_status = [\"trunc\", \"solo\", \"single\", \"paired\", \"fragmented\", \"shrunked\", \"complete\"]\n",
    "    count[list_status.index(status)] -= 1\n",
    "    return count\n",
    "\n",
    "def ClassLTR(hit, consensus_length, consensus_hits, count):\n",
    "    if CheckComplete(hit, consensus_length):\n",
    "        count[6] += 1\n",
    "        return [hit], \"complete\", count\n",
    "    elif CheckShrunked(hit, consensus_length):            # peut-être clippé\n",
    "        count[5] += 1\n",
    "        return [hit], \"shrunked\", count\n",
    "    elif CheckLTR(hit, consensus_length):\n",
    "        if CheckPaired(hit, consensus_length, consensus_hits):\n",
    "            TE_hits = BuildTE(hit, consensus_length, consensus_hits)\n",
    "            if CheckFragmented(TE_hits, consensus_length):\n",
    "                count[4] += 1\n",
    "                return TE_hits, \"fragmented\", count\n",
    "            else:\n",
    "                count[3] += 1\n",
    "                return TE_hits, \"paired\", count\n",
    "        elif CheckSingle(hit, consensus_length, consensus_hits):\n",
    "            TE_hits = BuildTE(hit, consensus_length, consensus_hits)\n",
    "            count[2] += 1\n",
    "            return TE_hits, \"single\", count\n",
    "        else:\n",
    "            count[1] += 1\n",
    "            return [hit], \"solo\", count\n",
    "    else:\n",
    "        count[0] += 1\n",
    "        return [], \"trunc\", count\n",
    "\n",
    "def CheckComplete(hit, consensus_length):\n",
    "    length_hit = int(hit[\"contig_stop\"]) - int(hit[\"contig_start\"])\n",
    "    return length_hit >= 0.95*consensus_length[\"complete\"]\n",
    "\n",
    "def CheckShrunked(hit, consensus_length):\n",
    "    coord_5LTR = 0.5*consensus_length[\"5-LTR\"]\n",
    "    coord_3LTR = consensus_length[\"complete\"] - 0.5*consensus_length[\"3-LTR\"]\n",
    "    if hit[\"strand\"] == \"+\":\n",
    "        return int(hit[\"TE_start\"]) < coord_5LTR and int(hit[\"TE_stop\"]) > coord_3LTR\n",
    "    else:\n",
    "        return int(hit[\"TE_stop\"]) < coord_5LTR and int(hit[\"TE_start\"]) > coord_3LTR\n",
    "\n",
    "def CheckLTR(hit, consensus_length):\n",
    "    if hit[\"strand\"] == \"+\" :\n",
    "        if int(hit[\"TE_start\"]) <= consensus_length[\"5-LTR\"] \\\n",
    "        or int(hit[\"TE_stop\"]) >= consensus_length[\"complete\"]-consensus_length[\"3-LTR\"]:\n",
    "            return int(hit[\"TE_stop\"])-int(hit[\"TE_start\"]) >= 0.5*consensus_length[\"5-LTR\"] \\\n",
    "                and int(hit[\"perc_id\"]) >= 75\n",
    "        else:\n",
    "            return False\n",
    "    else:\n",
    "        if int(hit[\"TE_stop\"]) <= consensus_length[\"5-LTR\"] \\\n",
    "        or int(hit[\"TE_start\"]) >= consensus_length[\"complete\"]-consensus_length[\"3-LTR\"]:\n",
    "            return int(hit[\"TE_start\"])-int(hit[\"TE_stop\"]) >= 0.5*consensus_length[\"5-LTR\"] \\\n",
    "                and int(hit[\"perc_id\"]) >= 75\n",
    "        else:\n",
    "            return False\n",
    "\n",
    "def CheckPaired(query_hit, consensus_length, consensus_hits):\n",
    "    if SearchSense(query_hit[\"strand\"], int(query_hit[\"TE_start\"]), int(query_hit[\"TE_stop\"]), consensus_length[\"5-LTR\"]):          # change due to error of pairing with itself, so if we have the 5-LTR\n",
    "        if query_hit[\"strand\"] == \"+\" :\n",
    "            coord_start = int(query_hit[\"contig_start\"]) - int(query_hit[\"TE_start\"])\n",
    "        else:\n",
    "            coord_start = int(query_hit[\"contig_start\"]) + int(query_hit[\"TE_start\"]) - consensus_length[\"complete\"]\n",
    "        coord_min = coord_start + consensus_length[\"complete\"] - (1.2*consensus_length[\"3-LTR\"])\n",
    "        coord_max = coord_start + consensus_length[\"complete\"] + (1.2*consensus_length[\"3-LTR\"])\n",
    "        if query_hit[\"contig\"] == \"NW_024067682.1\":\n",
    "            print(query_hit[\"index\"], coord_min, coord_max)\n",
    "        for other_hit in consensus_hits:\n",
    "            if coord_min <= int(other_hit[\"contig_stop\"]) <= coord_max \\\n",
    "            and query_hit[\"contig\"] == other_hit[\"contig\"] \\\n",
    "            and query_hit[\"index\"] != other_hit[\"index\"]:               # added for smaller TE paired anyway (linked with suppression of coord_min)\n",
    "                if CheckLTR(other_hit, consensus_length):\n",
    "                    return True\n",
    "    else:                                                                   # else we have the 3-LTR\n",
    "        if query_hit[\"strand\"] == \"+\" :\n",
    "            coord_start = int(query_hit[\"contig_stop\"]) - int(query_hit[\"TE_stop\"]) + consensus_length[\"complete\"]\n",
    "        else:\n",
    "            coord_start = int(query_hit[\"contig_stop\"]) + int(query_hit[\"TE_stop\"])\n",
    "        coord_min = coord_start - consensus_length[\"complete\"] - (1.2*consensus_length[\"3-LTR\"])\n",
    "        coord_max = coord_start - consensus_length[\"complete\"] + (1.2*consensus_length[\"3-LTR\"])\n",
    "        for other_hit in consensus_hits:\n",
    "            if coord_min <= int(other_hit[\"contig_start\"]) <= coord_max \\\n",
    "            and query_hit[\"contig\"] == other_hit[\"contig\"] \\\n",
    "            and query_hit[\"index\"] != other_hit[\"index\"]:               # added for smaller TE paired anyway (linked with suppression of coord_max)\n",
    "                if CheckLTR(other_hit, consensus_length):\n",
    "                    return True\n",
    "    return False\n",
    "\n",
    "def CheckFragmented(TE_hits, consensus_length):\n",
    "    length_covered = 0\n",
    "    for hit in TE_hits:\n",
    "        if length_covered == 0:\n",
    "            length_covered = int(hit[\"contig_stop\"]) - int(hit[\"contig_start\"])\n",
    "            coord_max = int(hit[\"contig_stop\"])\n",
    "        else:\n",
    "            length_covered += int(hit[\"contig_stop\"]) - max(coord_max, int(hit[\"contig_start\"]))\n",
    "    return length_covered >= 0.8*consensus_length[\"complete\"]\n",
    "\n",
    "def CheckSingle(query_hit, consensus_length, consensus_hits):\n",
    "    length_hit = int(hit[\"contig_stop\"]) - int(hit[\"contig_start\"])\n",
    "    if length_hit >= consensus_length[\"5-LTR\"]+500:\n",
    "        return True\n",
    "    else:\n",
    "        if SearchSense(query_hit[\"strand\"], int(query_hit[\"TE_start\"]), int(query_hit[\"TE_stop\"]), consensus_length[\"5-LTR\"]):           # change due to error of pairing with itself\n",
    "            if query_hit[\"strand\"] == \"+\" :\n",
    "                coord_start = int(query_hit[\"contig_start\"]) - int(query_hit[\"TE_start\"]) + consensus_length[\"5-LTR\"]\n",
    "            else:\n",
    "                coord_start = int(query_hit[\"contig_start\"]) + int(query_hit[\"TE_start\"]) - consensus_length[\"complete\"] + consensus_length[\"5-LTR\"]\n",
    "            for other_hit in consensus_hits:\n",
    "                if coord_start <= int(other_hit[\"contig_start\"]) <= coord_start+500 \\\n",
    "                and query_hit[\"contig\"] == other_hit[\"contig\"]:\n",
    "                    return True\n",
    "        else:\n",
    "            if query_hit[\"strand\"] == \"+\" :\n",
    "                coord_start = int(query_hit[\"contig_stop\"]) - int(query_hit[\"TE_stop\"]) + consensus_length[\"complete\"] - consensus_length[\"5-LTR\"]\n",
    "            else:\n",
    "                coord_start = int(query_hit[\"contig_stop\"]) + int(query_hit[\"TE_stop\"]) - consensus_length[\"5-LTR\"]\n",
    "            for other_hit in consensus_hits:\n",
    "                if coord_start-500 <= int(other_hit[\"contig_stop\"]) <= coord_start \\\n",
    "                and query_hit[\"contig\"] == other_hit[\"contig\"]:\n",
    "                    return True\n",
    "        return False\n",
    "\n",
    "def BuildTE(query_hit, consensus_length, consensus_hits):\n",
    "    TE_hits = []\n",
    "    if SearchSense(query_hit[\"strand\"], int(query_hit[\"TE_start\"]), int(query_hit[\"TE_stop\"]), consensus_length[\"5-LTR\"]):           # change due to error of pairing with itself\n",
    "        if query_hit[\"strand\"] == \"+\" :\n",
    "            coord_start = int(query_hit[\"contig_start\"]) - int(query_hit[\"TE_start\"])\n",
    "        else:\n",
    "            coord_start = int(query_hit[\"contig_start\"]) + int(query_hit[\"TE_start\"]) - consensus_length[\"complete\"]\n",
    "        for other_hit in consensus_hits:\n",
    "            # print(other_hit[\"contig_start\"])\n",
    "            if coord_start <= int(other_hit[\"contig_start\"]) <= coord_start+1.2*consensus_length[\"complete\"] \\\n",
    "            and query_hit[\"contig\"] == other_hit[\"contig\"]:\n",
    "                TE_hits.append(other_hit)\n",
    "    else:\n",
    "        if query_hit[\"strand\"] == \"+\" :\n",
    "            coord_start = int(query_hit[\"contig_stop\"]) - int(query_hit[\"TE_stop\"]) + consensus_length[\"complete\"]\n",
    "        else:\n",
    "            coord_start = int(query_hit[\"contig_stop\"]) + int(query_hit[\"TE_stop\"])\n",
    "        for other_hit in consensus_hits:\n",
    "            if coord_start-1.2*consensus_length[\"complete\"] <= int(other_hit[\"contig_stop\"]) <= coord_start \\\n",
    "            and query_hit[\"contig\"] == other_hit[\"contig\"]:\n",
    "                TE_hits.append(other_hit)\n",
    "    return TE_hits\n",
    "\n",
    "\n",
    "\n",
    "os.chdir(sequences_dir)\n",
    "headers = [\"index\", \"contig\", \"start\", \"stop\", \"length\", \"strand\", \"id_percentage\", \"TE_start\", \"TE_stop\", \"classification\", \"index\", \"start\", \"stop\", \"length\", \"strand\", \"id_percentage\", \"TE_start\", \"TE_stop\"]\n",
    "dico_count = {}\n",
    "\n",
    "for consensus in dico_repeat_hits :\n",
    "    print(consensus)\n",
    "    os.chdir(consensus)\n",
    "    output_consensus = open(consensus + \".LTR_classif.tsv\", \"w\")\n",
    "    output_consensus.write(\"\\t\".join(headers) + \"\\n\")\n",
    "    count = [0,0,0,0,0,0,0]\n",
    "    hits_used = []\n",
    "    list_lines = []\n",
    "    \n",
    "    for hit in dico_repeat_hits[consensus]:\n",
    "        if hit in hits_used:\n",
    "            continue\n",
    "        else:\n",
    "            TE_hits, status, count = ClassLTR(hit, dico_length[consensus], dico_repeat_hits[consensus], count)\n",
    "            ref_line = [str(hit[\"index\"]), hit[\"contig\"], hit[\"contig_start\"], hit[\"contig_stop\"], str(int(hit[\"contig_stop\"])-int(hit[\"contig_start\"])), hit[\"strand\"], str(hit[\"perc_id\"]), hit[\"TE_start\"], hit[\"TE_stop\"], status]\n",
    "            list_lines.append(ref_line)\n",
    "            if len(TE_hits) == 0 :\n",
    "                list_lines.append([\"\\tno_match\\n\"])\n",
    "            else:\n",
    "                new_line = ''\n",
    "                for other_hit in TE_hits:\n",
    "                    hits_used.append(other_hit)\n",
    "                    for line in list_lines:\n",
    "                        if str(other_hit[\"index\"]) == line[0] and line != ref_line:\n",
    "                            del list_lines[list_lines.index(line):list_lines.index(line)+2]\n",
    "                            count = FixCount(line[-1], count)\n",
    "                    line = [new_line, str(other_hit[\"index\"]), other_hit[\"contig_start\"], other_hit[\"contig_stop\"], str(int(other_hit[\"contig_stop\"])-int(other_hit[\"contig_start\"])), other_hit[\"strand\"], str(other_hit[\"perc_id\"]), other_hit[\"TE_start\"], other_hit[\"TE_stop\"], \"\\n\"]\n",
    "                    list_lines.append(line)\n",
    "                    new_line = 9*'\\t'\n",
    "\n",
    "    for line in list_lines:\n",
    "        output_consensus.write(\"\\t\".join(line))\n",
    "    output_consensus.close()\n",
    "    dico_count[consensus] = count\n",
    "    os.chdir(\"..\")\n",
    "\n",
    "output_count = open(\"annotation_counts.tsv\", \"w\")\n",
    "headers = [\"consensus\", \"length\", \"trunc\", \"solo\", \"single\", \"paired\", \"fragmented\", \"shrunked\", \"complete\"]\n",
    "output_count.write(\"\\t\".join(headers) + \"\\n\")\n",
    "\n",
    "for consensus in dico_count:\n",
    "    output_count.write(consensus + \"\\t\" + str(dico_length[consensus][\"complete\"]) + \"\\t\" + \"\\t\".join(str(i) for i in dico_count[consensus]) + \"\\n\")   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_count = open(\"annotation_counts.tsv\", \"w\")\n",
    "headers = [\"consensus\", \"length\", \"trunc\", \"solo\", \"single\", \"paired\", \"fragmented\", \"shrunked\", \"complete\"]\n",
    "output_count.write(\"\\t\".join(headers) + \"\\n\")\n",
    "\n",
    "for consensus in dico_count:\n",
    "    output_count.write(consensus + \"\\t\" + str(dico_length[consensus][\"complete\"]) + \"\\t\" + \"\\t\".join(str(i) for i in dico_count[consensus]) + \"\\n\")   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4 : Extraction of LTR coordinates and sequences\n",
    "After the classification, the goal of this program is to identify the age of insertion of each TE for each consensus. We need the TE sequences for it, and so TE coordinates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# structure of dico_LTRs_by_consensus :\n",
    "# { consensus1 : [LTR_list], consensus2 : [LTR_list], ...}\n",
    "# LTR_list = [{LTR1_hit1}, {LTR1_hit2}, {LTR15_hit1}, {LTR15_hit2}, ...]\n",
    "# LTR1_hit1 = {id : index_status_5LTR, contig, contig_start, contig_stop, strand, id_percentage, TE_start, TE_stop}\n",
    "# then we will add the sequence to that dictionnary\n",
    "\n",
    "def Check5LTR(TE_start, TE_stop, strand, consensus_length):\n",
    "    '''Check if the hit can be considered as a 5'LTR from its coordinates and its coverage of LTR'''\n",
    "    if strand == \"+\":\n",
    "        return TE_start < consensus_length[\"5-LTR\"] \\\n",
    "           and min(TE_stop,consensus_length[\"5-LTR\"]) - TE_start > 0.8*consensus_length[\"5-LTR\"]\n",
    "    else:\n",
    "        return TE_stop < consensus_length[\"5-LTR\"]\\\n",
    "           and min(TE_start,consensus_length[\"5-LTR\"]) - TE_stop > 0.8*consensus_length[\"5-LTR\"]\n",
    "\n",
    "def Check3LTR(TE_start, TE_stop, strand, consensus_length):\n",
    "    '''Check if the hit can be considered as a 3'LTR from its coordinates and its coverage of LTR'''\n",
    "    if strand == \"+\":\n",
    "        return TE_stop > consensus_length[\"complete\"] - consensus_length[\"3-LTR\"] \\\n",
    "           and TE_stop - max(TE_start,consensus_length[\"complete\"]-consensus_length[\"3-LTR\"]) > 0.5*consensus_length[\"3-LTR\"]\n",
    "    else:\n",
    "        return TE_start > consensus_length[\"complete\"] - consensus_length[\"3-LTR\"] \\\n",
    "           and TE_start - max(TE_stop,consensus_length[\"complete\"]-consensus_length[\"3-LTR\"]) > 0.5*consensus_length[\"3-LTR\"]\n",
    "\n",
    "def CountRepetitions(list_hits, id):\n",
    "    '''Some hits are considered as the same LTR for the same TE (possible overlapping between several TEs,\n",
    "    considered as one TE). This function manage those repetitions by giving a new index with a number\n",
    "    (ex : 15436_complete_5LTR.2)'''\n",
    "    i=1\n",
    "    new_id = id\n",
    "    for hit in list_hits:\n",
    "        if id in hit[\"id\"]:\n",
    "            i+=1\n",
    "            new_id = id + \".\" + str(i)\n",
    "    return new_id\n",
    "\n",
    "def AddLTR(elements, id, contig):\n",
    "    '''Add the LTR informations in dico_LTRs_by_consensus'''\n",
    "    return {\"id\":id, \"contig\":contig,\n",
    "            \"contig_start\":elements[11],\n",
    "            \"contig_stop\":elements[12],\n",
    "            \"strand\":elements[14],\n",
    "            \"id_percentage\":elements[15],\n",
    "            \"TE_start\":elements[16],\n",
    "            \"TE_stop\":elements[17] }\n",
    "\n",
    "def WriteLTR(getfasta_library, dico_LTR, consensus_length, contig_length):\n",
    "    '''Write the coordinates file used by bedtools getfasta to generate fasta files.\n",
    "    Coordinates are taking the entire predicted LTR and not the entire hit.\n",
    "    coordinate_start = estimated minimal LTR coordinate (5' start or 3' stop) or first contig base\n",
    "    coordinate_stop = estimated maximal LTR coordinate (5' stop or 3' start) or last contig base'''\n",
    "    print(id)\n",
    "    if dico_LTR[\"strand\"] == \"+\":\n",
    "        if \"5LTR\" in dico_LTR[\"id\"]:\n",
    "            coordinate_start = max(int(dico_LTR[\"contig_start\"]) - int(dico_LTR[\"TE_start\"]) - 50, 1)\n",
    "            coordinate_stop = min(int(dico_LTR[\"contig_stop\"]), coordinate_start+consensus_length[\"5-LTR\"]+100)\n",
    "        else:\n",
    "            coordinate_stop = min(int(dico_LTR[\"contig_stop\"]) + consensus_length[\"complete\"] - int(dico_LTR[\"TE_stop\"]) + 50, contig_length)\n",
    "            coordinate_start = max(int(dico_LTR[\"contig_start\"]), coordinate_stop-consensus_length[\"3-LTR\"]-50)\n",
    "    else:\n",
    "        dico_LTR[\"strand\"] = \"-\"\n",
    "        if \"5LTR\" in dico_LTR[\"id\"]:\n",
    "            coordinate_stop = min(int(dico_LTR[\"contig_stop\"]) + int(dico_LTR[\"TE_stop\"]) + 50, contig_length)\n",
    "            coordinate_start = max(int(dico_LTR[\"contig_start\"]), coordinate_stop-consensus_length[\"5-LTR\"]-50)\n",
    "        else:\n",
    "            coordinate_start = max(int(dico_LTR[\"contig_start\"]) - consensus_length[\"complete\"] + int(dico_LTR[\"TE_start\"]) - 50, 1)\n",
    "            coordinate_stop = min(int(dico_LTR[\"contig_stop\"]), coordinate_start+consensus_length[\"3-LTR\"]+50)\n",
    "    line = [dico_LTR[\"contig\"], str(coordinate_start), str(coordinate_stop), dico_LTR[\"id\"], \"0\", dico_LTR[\"strand\"]]\n",
    "    getfasta_library.write(\"\\t\".join(line) + \"\\n\")\n",
    "    # remember the columns in bed files :\n",
    "    # contig    contig_start    contig_stop     name    (score)   strand\n",
    "\n",
    "# Taking fasta sequences for all contigs of the genome for their length (to avoid out of bounds TEs)\n",
    "contigs = {seq_record.id: seq_record.seq for seq_record in SeqIO.parse(genome, \"fasta\")}\n",
    "for contig in contigs:\n",
    "    dico_length[contig] = len(contigs[contig])\n",
    "\n",
    "os.chdir(sequences_dir)\n",
    "dico_LTRs_by_consensus = {}\n",
    "for consensus in os.listdir(\".\"):\n",
    "    getfasta_library = open(consensus + \"/\" + consensus + \".LTR_coordinates.bed\", \"w\")\n",
    "    print(consensus, dico_length[consensus])\n",
    "    dico_LTRs_by_consensus[consensus] = []\n",
    "    for line in open(consensus + \"/\" + consensus + \".LTR_classif.tsv\", \"r\"):\n",
    "        elements = line.split(\"\\t\")\n",
    "        # skip trunc TEs and header\n",
    "        if elements[10] == \"no_match\\n\" or elements[9] == \"classification\":\n",
    "            continue\n",
    "        # elif elements[0] == \"\":\n",
    "        #     dico_one_LTR = {}\n",
    "        #     if Check5LTR and dico_one_LTR[\"5-LTR\"] != {}:\n",
    "        #         doc_one_LTR[\"5-LTR\"] = {\"contig\", \"contig-start\", \"contig-stop\", \"strand\", \"TE-start\", \"TE-stop\"}\n",
    "        #     elif Check3LTR and dico_one_LTR[\"3-LTR\"] != {}:\n",
    "        #         doc_one_LTR[\"3-LTR\"] = {\"contig\", \"contig-start\", \"contig-stop\", \"strand\", \"TE-start\", \"TE-stop\"}\n",
    "        else:\n",
    "            # informations for a new TE\n",
    "            if elements[0] != \"\":\n",
    "                index = elements[0]         # index = line of hit in RepeatMasker file\n",
    "                contig = elements[1]\n",
    "                status = elements[9]        # status = LTR classification\n",
    "            if Check5LTR(int(elements[16]), int(elements[17]), elements[14], dico_length[consensus]):\n",
    "                # id will be used to identify LTRs in trees\n",
    "                id = CountRepetitions(dico_LTRs_by_consensus[consensus], \"%s_%s_5LTR\" % (index, status))\n",
    "                dico_LTRs_by_consensus[consensus].append(AddLTR(elements, id, contig))\n",
    "                WriteLTR(getfasta_library, dico_LTRs_by_consensus[consensus][-1], dico_length[consensus], dico_length[contig])\n",
    "            if Check3LTR(int(elements[16]), int(elements[17]), elements[14], dico_length[consensus]):       # no elif because complete TEs have both LTR\n",
    "                id = CountRepetitions(dico_LTRs_by_consensus[consensus], \"%s_%s_3LTR\" % (index, status))\n",
    "                dico_LTRs_by_consensus[consensus].append(AddLTR(elements, id, contig))\n",
    "                WriteLTR(getfasta_library, dico_LTRs_by_consensus[consensus][-1], dico_length[consensus], dico_length[contig])\n",
    "    getfasta_library.close()            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5 : Phylogeny of TEs\n",
    "This step is done after a run of Extract_LTR_for_phylogeny.sh, which run bedtools getfasta and mafft.\n",
    "Those runs need also to convert the alignments in .nex files (here done with Mesquite)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 c64_s1\n",
      "2 c399_s1\n",
      "3 c111_s2\n",
      "4 c82_s1\n",
      "5 c79_s1\n",
      "6 c60_s1\n",
      "7 c339_s1\n",
      "8 c64_s2\n",
      "9 c15_s1\n",
      "10 c339_s2\n",
      "11 c34_s2\n",
      "12 c411\n",
      "13 c124_s2\n",
      "14 c384_s2\n",
      "15 c117_s1\n",
      "16 c79_s2\n",
      "17 c384_s1\n",
      "18 c46_s1\n",
      "19 c419\n",
      "20 c53_s2\n",
      "21 c399_s2\n",
      "22 c46_s2\n",
      "23 c366\n",
      "24 c320\n",
      "25 c43_s1\n",
      "26 c60_s2\n",
      "27 c83_s2\n",
      "28 c117_s2\n",
      "29 c221_s2\n",
      "30 c43_s2\n",
      "31 c130_s2\n",
      "32 c83_s1\n",
      "33 c8_s2\n",
      "34 c133\n",
      "35 c194\n",
      "36 c332\n",
      "37 c15_s2\n",
      "38 c71_s2\n",
      "39 c111_s1\n",
      "40 c347_s2\n",
      "41 c53_s1\n",
      "42 c62_s1\n",
      "43 c71_s1\n",
      "44 c204_s1\n",
      "45 c57_s2\n",
      "46 c244\n",
      "47 c347_s1\n",
      "48 c144_s2\n",
      "49 c242_s1\n",
      "50 c62_s2\n",
      "51 c124_s1\n",
      "52 c144_s1\n",
      "53 c82_s2\n",
      "54 c304\n",
      "55 c361\n",
      "56 c103_s2\n",
      "57 c336_s1\n",
      "58 c215_s1\n",
      "59 c145\n",
      "60 c226\n",
      "61 c165_s1\n",
      "62 c21_s2\n",
      "63 c354_s1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "hwloc/linux: Ignoring PCI device with non-16bit domain.\n",
      "Pass --enable-32bits-pci-domain to configure to support such devices\n",
      "(warning: it would break the library ABI, don't enable unless really needed).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64 c113\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "hwloc/linux: Ignoring PCI device with non-16bit domain.\n",
      "Pass --enable-32bits-pci-domain to configure to support such devices\n",
      "(warning: it would break the library ABI, don't enable unless really needed).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "65 c57_s1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "hwloc/linux: Ignoring PCI device with non-16bit domain.\n",
      "Pass --enable-32bits-pci-domain to configure to support such devices\n",
      "(warning: it would break the library ABI, don't enable unless really needed).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "66 c101_s1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "hwloc/linux: Ignoring PCI device with non-16bit domain.\n",
      "Pass --enable-32bits-pci-domain to configure to support such devices\n",
      "(warning: it would break the library ABI, don't enable unless really needed).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "67 c290_s1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "hwloc/linux: Ignoring PCI device with non-16bit domain.\n",
      "Pass --enable-32bits-pci-domain to configure to support such devices\n",
      "(warning: it would break the library ABI, don't enable unless really needed).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "68 c3_s2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "hwloc/linux: Ignoring PCI device with non-16bit domain.\n",
      "Pass --enable-32bits-pci-domain to configure to support such devices\n",
      "(warning: it would break the library ABI, don't enable unless really needed).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "69 c157_s2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "hwloc/linux: Ignoring PCI device with non-16bit domain.\n",
      "Pass --enable-32bits-pci-domain to configure to support such devices\n",
      "(warning: it would break the library ABI, don't enable unless really needed).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "70 c55_s2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "hwloc/linux: Ignoring PCI device with non-16bit domain.\n",
      "Pass --enable-32bits-pci-domain to configure to support such devices\n",
      "(warning: it would break the library ABI, don't enable unless really needed).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "71 c67_s1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "hwloc/linux: Ignoring PCI device with non-16bit domain.\n",
      "Pass --enable-32bits-pci-domain to configure to support such devices\n",
      "(warning: it would break the library ABI, don't enable unless really needed).\n"
     ]
    }
   ],
   "source": [
    "os.chdir(sequences_dir)\n",
    "\n",
    "def RunMrBayes(consensus, count):\n",
    "    '''Write the nexus file used to run MrBayes, and then proceed to run MrBayes in parallel\n",
    "    version with 10 processors (need to be installed to shorten runs)'''\n",
    "    os.chdir(consensus)\n",
    "    nexus_file = \"mrbayes_commands.nex\"\n",
    "    mrbayes_script = open(nexus_file, \"w\")\n",
    "    file = \"\\texecute \" + consensus + \".LTR_alignment.nex;\"\n",
    "    # The number of generations is generated proportionally to the number of LTRs sequences\n",
    "    generations = \"\\tmcmc ngen=%s samplefreq=100 printfreq=100 diagnfreq=1000 nchains=5;\" % str(count*1000)\n",
    "    list_lines = [\"#NEXUS\\n\",\n",
    "                  \"begin mrbayes;\",\n",
    "                  file,\n",
    "                  \"\\tprset brlenspr=clock:uniform;\",\n",
    "                  \"\\tlset nst=2 rates=invgamma;\",\n",
    "                  generations,\n",
    "                  \"\\tsump;\",\n",
    "                  \"\\tsumt;\",\n",
    "                  \"end;\"]\n",
    "    mrbayes_script.write(\"\\n\".join(list_lines))\n",
    "    mrbayes_script.close()\n",
    "    os.system(\"mpirun -np 10 %s %s > mrbayes_run.log\" % (mb, nexus_file))\n",
    "    os.chdir(\"..\")\n",
    "\n",
    "# Get LTRs count for the number of MrBayes generations\n",
    "count_file = open(\"../annotation_counts.tsv\", \"r\")\n",
    "list_counts = []\n",
    "for line in count_file:\n",
    "    if \"paired\" not in line:\n",
    "        elements = line.split(\"\\t\")\n",
    "        list_counts.append([elements[0], int(elements[-1][:-1])])\n",
    "\n",
    "list_counts.sort(key=lambda x: (x[1]))      # shorter analysis will be done first\n",
    "i=0\n",
    "for consensus in list_counts:\n",
    "    i+=1\n",
    "    print(i, consensus[0])\n",
    "    if i>62:                # this line will be used if you want to stop your analysis\n",
    "        RunMrBayes(consensus[0], consensus[1])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step X : Context of insertion\n",
    "\n",
    "- gene context (most important)\n",
    "- recombination context\n",
    "- TF context\n",
    "- GC content\n",
    "- non B conformation\n",
    "- maybe methylation and polymorphism with flowcell"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "blast",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
